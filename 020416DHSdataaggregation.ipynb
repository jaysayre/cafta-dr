{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DHSdataaggregation.py - Takes DHS data and\n",
    "aggregates the variables in the files (most importantly income and wage data) to the occupation and municipality level, after making necessary cleaning and merging steps\n",
    "\n",
    "Jay Sayre - sayrejay (at) gmail,\n",
    "\n",
    "INPUTS: \n",
    "\n",
    "\"../DHS/2013Standard/geo/merge2013clust.csv\" - DHS 2013 Dominican Republic geospatial data corresponding to geo-tagged keys in DHS 2013 dataset, compiled by extractoneshapefiletopoint.py\n",
    "\n",
    "\"../DHS/2013Standard/hhmember/DRPR61FL.csv\" - DHS 2013 Dominican Republic Household Member Dataset (converted to csv by DHS/DHSbuild.R script)\n",
    "\n",
    "Ditto for DHS 2007 data (don't think I'm using this)\n",
    "\n",
    "\"/cafta-dr/DHS/DHSoccupationsISIC.xlsx\" - Code for converting DHS occupation categories into ISIC occupation codes, compiled by author (me!)\n",
    "\n",
    "usd2013todr13pesos - Nominal conversion rate from 2013 USD to 2013 RD, times a weighted factor wrt to \n",
    "current 2010 USD (to make this in real terms).\n",
    "\n",
    "INTERMEDIATE DATA: N/A\n",
    "\n",
    "OUTPUTS: \"averageincomebymunicipality2013.csv\" - Average income in each D.R. municipality for 2013\n",
    "\n",
    "\"averageincomebyoccmun2013.csv\" - Average income in each D.R. municipality and occupation for 2013\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if os.name == 'nt':\n",
    "    base_dir = \"D:/Dropbox/Dropbox (Personal)/College/DR_Paper/\"\n",
    "else:\n",
    "    base_dir = \"/home/j/Dropbox/College/DR_Paper/\"\n",
    "    \n",
    "## INPUTS\n",
    "dhs_geoclust_2007 = base_dir + \"DHS/2007Standard/geo/merge2007clust.csv\"\n",
    "dhs_hhmember_2007 = base_dir + \"DHS/2007Standard/hhmember/DRPR52FL.csv\" # Converted to csv by DHSbuild.R in  main directory\n",
    "dhs_geoclust_2013 = base_dir + \"DHS/2013Standard/geo/merge2013clust.csv\"\n",
    "dhs_hhmember_2013 = base_dir + \"DHS/2013Standard/hhmember/DRPR61FL.csv\" # Converted to csv by DHSbuild.R in  main directory\n",
    "dhs_occupations_conversion = base_dir+\"cafta-dr/DHS/DHSoccupationsISIC.xlsx\"\n",
    "## Conversion rate from 2013 RD to 2013 USD \n",
    "usd2013todr13pesos = 41.8081439153#*(0.985) ## Later term is so currency is wrt to 2010 constant USD, from World Bank\n",
    "\n",
    "## OUTPUTS\n",
    "averageincmun = base_dir + \"/cafta-dr/Output/averageincomebymunicipality2013.csv\"\n",
    "averageincmunocc = base_dir + \"/cafta-dr/Output/averageincomebyoccmun2013.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes the DHS geocluster csvs (merged with ONE municipality codes by 091215extractoneshapefiletopoint.py and converted to csvs by 091215dbftodata.py) and merges them with DHS samples\n",
    "\n",
    "Number of DHS clusters incorrectly marked in another province: 53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total observations 41267\n",
      "Observations with occcupation data 8487\n",
      "Observations classified as having a private employer 3281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/j/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2902: DtypeWarning: Columns (503,509,511,547,550,553,555,558,559,634,635,637,708,709,711,787,788,790,811,837,838,839) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#geo_df_2007 = pd.read_csv(dhs_geoclust_2007)\n",
    "#dhs_data_2007 = pd.read_csv(dhs_hhmember_2007)\n",
    "geo_df_2013 = pd.read_csv(dhs_geoclust_2013, encoding='latin_1')\n",
    "dhs_data_2013 = pd.read_csv(dhs_hhmember_2013, encoding='latin_1')\n",
    "\n",
    "## Subset geocluster datasets down to relevant variables\n",
    "geo_keep_cols = ['ADM1DHS','ALT_DEM',\"DHSCLUST\",\"MUN\",\"PROV\",\"REG\",\"URBAN_RURA\"]\n",
    "#geo_df_2007 = geo_df_2007[geo_keep_cols]\n",
    "geo_df_2013 = geo_df_2013[geo_keep_cols]\n",
    "## Merge geodata with DHS data\n",
    "#dhs_data_2007 = dhs_data_2007.merge(geo_df_2007, left_on=\"hv001\", right_on=\"DHSCLUST\", how=\"left\")\n",
    "dhs_data_2013 = dhs_data_2013.merge(geo_df_2013, left_on=\"hv001\", right_on=\"DHSCLUST\", how=\"left\")\n",
    "\n",
    "###May want to apply some correction to municipality displacement. Only problem is correction isn't possible for 2013 data.\n",
    "###Commented section below tests whether different shapefile has better municipality displacement, which it does not.\n",
    "#count = 0 \n",
    "#for i in geo_df_2013.index:\n",
    "#    if geo_df_2013['ADM1DHS'][i] != geo_df_2013['PROV'][i]:\n",
    "#        count += 1\n",
    "#print \"Number of DHS clusters incorrectly marked in another province:\", count\n",
    "\n",
    "### Clean DHS data\n",
    "\n",
    "## Convert occupations provided in espanol to ISIC two digit code\n",
    "## Build conversion dictionary\n",
    "occonversion = pd.read_excel(dhs_occupations_conversion,encoding='latin_1')\n",
    "occonversion['isic2digitV3']=occonversion['isic2digitV3'].astype(str)\n",
    "occonversion['isic2digitV3']=occonversion['isic2digitV3'].apply(lambda x: x.split(','))\n",
    "occonversion = dict(zip(occonversion['espanol'],occonversion['isic2digitV3']))\n",
    "occonversion['missing'] = '99'\n",
    "## Take steps necessary to convert occupations\n",
    "dhs_data_2013['sg110'] = dhs_data_2013['sg110'].fillna('missing')\n",
    "dhs_data_2013['sg110'] = dhs_data_2013['sg110'].apply(lambda x: occonversion[x.rstrip()])\n",
    "print \"Total observations\", len(dhs_data_2013['sg110'])\n",
    "print \"Observations with occcupation data\", len(dhs_data_2013[dhs_data_2013['sg110'] != '99'])\n",
    "\n",
    "## Subset down to workers employed in the private sector \n",
    "## NOT SURE IF I'M  DOING THIS, BUT I DID IT IN IPUMS DATA \n",
    "##Do I want to include the 162 some employers here? 'employer'\n",
    "print \"Observations classified as having a private employer\", len(dhs_data_2013[dhs_data_2013['sg111'] == \"private employee\"])\n",
    "#dhs_data_2013 = dhs_data_2013[dhs_data_2013['sg111'] == \"private employee\"] \n",
    "\n",
    "### Convert income to unified period, convert to 2013 USD\n",
    "## Convert income to correct periodicity, defaults to assuming pay is monthly since it is most common\n",
    "grosssalary_period_conv = {\"monthly\":12,'by weekly':26,'weekly':52,'yearly':1,\"don't know\":12}\n",
    "frstsourcinc_period_conv = {\"monthly\":12,'by weekly':26,'weekly':52,'yearly':1,\"don't know\":12}\n",
    "\n",
    "dhs_data_2013['sg112a'] = dhs_data_2013['sg112a'].apply(lambda x: x if x != 999998 else 0)\n",
    "dhs_data_2013['sg112b'] = dhs_data_2013['sg112b'].apply(lambda x: grosssalary_period_conv.get(x,12))\n",
    "dhs_data_2013['sg117b'] = dhs_data_2013['sg117b'].apply(lambda x: frstsourcinc_period_conv.get(x,12))\n",
    "\n",
    "## Convert income to 2013 USD\n",
    "dhs_data_2013['grossalary'] = (dhs_data_2013['sg112a']*dhs_data_2013['sg112b'])/usd2013todr13pesos\n",
    "dhs_data_2013['frstsourcinc'] = (dhs_data_2013['sg117a']*dhs_data_2013['sg117b'])/usd2013todr13pesos\n",
    "dhs_data_2013['occinc'] = (dhs_data_2013['singresoo']*12)/usd2013todr13pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations with income data 8244\n",
      "2998.33430696\n",
      "2990.59210909\n",
      "2505.69176996\n"
     ]
    }
   ],
   "source": [
    "##Check average income numbers for DR and if they match up with GDP P.C.\n",
    "##2013 -  5,968.7, 2002 - 3,008.5 \n",
    "##Takeaway is that there are several different sources for income data\n",
    "print \"Observations with income data\", len(dhs_data_2013[dhs_data_2013['sg112a'].astype(str) != 'nan'])\n",
    "\n",
    "print dhs_data_2013['occinc'].mean()\n",
    "print dhs_data_2013['grossalary'].mean()\n",
    "print dhs_data_2013['frstsourcinc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8244\n",
      "8187\n"
     ]
    }
   ],
   "source": [
    "def codigo(prov):\n",
    "    prov = str(prov)\n",
    "    if len(prov) == 1:\n",
    "        return '0'+prov\n",
    "    else:\n",
    "        return prov\n",
    "\n",
    "#dhs_data_2007 = dhs_data_2007[dhs_data_2007['ingoo'].astype(str) != 'nan']\n",
    "dhs_data_2013 = dhs_data_2013[dhs_data_2013['grossalary'].astype(str) != 'nan']\n",
    "columnstoaggregate = ['grossalary','occinc','frstsourcinc']\n",
    "\n",
    "## Aggregate income at the municipality level\n",
    "#### COULD SUBSET THIS DOWN TO ONLY ENTRIES WITH OCCUPATION DATA, AS BELOW\n",
    "muninc2013 = dhs_data_2013.groupby(['PROV','MUN'], as_index=False)[columnstoaggregate].mean()\n",
    "muninc2013['PROV'] = muninc2013['PROV'].astype(str)\n",
    "muninc2013['MUN'] = muninc2013['MUN'].apply(lambda x: codigo(x))\n",
    "muninc2013['CODIGO'] = muninc2013['PROV']+muninc2013['MUN']\n",
    "muninc2013.to_csv(averageincmun, index=False)\n",
    "\n",
    "## Once you subset down to only entries with income data, this only reduces entries by 100 or so \n",
    "print len(dhs_data_2013)\n",
    "dhs_data_2013 =  dhs_data_2013[dhs_data_2013['sg110'] != '99']\n",
    "print len(dhs_data_2013)\n",
    "\n",
    "## Aggregate income at the municipality and occupation level\n",
    "## Duplicate rows into different occupation categories\n",
    "dhs_data_2013 = dhs_data_2013.reset_index()\n",
    "dhs_data_2013.drop('index',1,inplace=True)\n",
    "origrows = list(dhs_data_2013.index)\n",
    "for i in origrows:\n",
    "    numisics = len(dhs_data_2013.loc[i,'sg110'])\n",
    "    if numisics != 1:\n",
    "        updaterows = []\n",
    "        for j in range(numisics)[1:]:\n",
    "            newrow = dict(dhs_data_2013.loc[i,:])\n",
    "            newrow['sg110']=dhs_data_2013.loc[i,'sg110'][j]\n",
    "            updaterows.append(newrow)\n",
    "        dhs_data_2013 = dhs_data_2013.append(updaterows)\n",
    "        dhs_data_2013 = dhs_data_2013.reset_index()\n",
    "        dhs_data_2013.drop('index',1,inplace=True)\n",
    "    \n",
    "    dhs_data_2013.loc[i,'sg110'] = dhs_data_2013.loc[i,'sg110'][0]\n",
    "\n",
    "\n",
    "dhs_data_2013 = dhs_data_2013.groupby(['PROV','MUN','sg110'], as_index=False)[columnstoaggregate].mean()\n",
    "dhs_data_2013['PROV'] = dhs_data_2013['PROV'].astype(str)\n",
    "dhs_data_2013['MUN'] = dhs_data_2013['MUN'].apply(lambda x: codigo(x))\n",
    "dhs_data_2013['CODIGO'] = dhs_data_2013['PROV']+dhs_data_2013['MUN']\n",
    "dhs_data_2013.to_csv(averageincmunocc, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
